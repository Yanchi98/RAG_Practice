{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b88919ac-dcd8-452e-89b5-6962e9f07e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_split_prompt = \"\"\"你是一个善于理解用户问题的助手，现在我要你帮助我将一个复杂问题拆解为多个子问题。请严格遵循以下步骤完成： \n",
    "步骤： \n",
    "1、理解问题的意图。 \n",
    "2、判断是简单问题还是复杂问题，判断依据为该问题是否只涉及单个实体，而不是多个实体的对比。如果是简单问题请直接输出结果，复杂问题则进入步骤3。\n",
    "3、分解问题： 将问题分解为可以单独回答的最小单元子问题。每个子问题都应尽量具体、独立且易于回答。确保通过整合所有子问题的答案可以推理得出复杂问题的答案。 \n",
    "4、输出子问题： 输出格式{“question”:, \"question_type\":\"\", “sub_quesiton”:[]}  \n",
    "\n",
    "例子1： \n",
    "问题: 苹果13的续航和华为mate60的续航谁更强？ \n",
    "输出：{“question”:“苹果13的续航和华为mate60的续航谁更强？”,\"question_type\":\"complex\", “sub_quesiton”:[“苹果13的续航是多少？”, “华为mate60的续航是多少？”]} \n",
    "例子2： \n",
    "问题: 苹果13的续航是多少？ \n",
    "输出：{“question”:“苹果13的续航是多少？”,\"question_type\":\"simple\", “sub_quesiton”:[]}\n",
    "\n",
    "现在请你拆解以下问题： 问题：苹果和华为谁更好？输出：\n",
    "\"\"\"\n",
    "\n",
    "index_prompt = \"\"\"你是一个问答小助手，你的任务是从搜索引擎返回的结果中抽取答案来回答问题，如果答案引用了某条检索结果，用下标[序号]来表示引用。\n",
    "问题：“什么是BERT?”\n",
    "检索结果：\n",
    "[1]Transformer模型（直译为“变换器”）是一种采用注意力机制的深度学习模型，这一机制可以按输入数据各部分重要性的不同而分配不同的权重。该模型主要用于自然语言处理（NLP）与计算机视觉（CV）领域。[1]\n",
    "与循环神经网络（RNN）一样，Transformer模型旨在处理自然语言等顺序输入数据，可应用于翻译、文本摘要等任务。而与RNN不同的是，Transformer模型能够一次性处理所有输入数据。注意力机制可以为输入序列中的任意位置提供上下文。如果输入数据是自然语言，则Transformer不必像RNN一样一次只处理一个单词，这种架构允许更多的并行计算，并以此减少训练时间。[2]\n",
    "Transformer模型于2017年由谷歌大脑的一个团队推出[2]，现已逐步取代长短期记忆（LSTM）等RNN模型成为了NLP问题的首选模型。[3]并行化优势允许其在更大的数据集上进行训练。这也促成了BERT、GPT等预训练模型的发展。[4][5] 这些系统使用了维基百科、Common Crawl等大型语料库进行训练，并可以针对特定任务进行微调。[6][7]\n",
    "[2]BERT(Bidirectional Encoder Representation from Transformers)是2018年10月由Google AI研究院提出的一种预训练模型，该模型在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩: 全部两个衡量指标上全面超越人类，并且在11种不同NLP测试中创出SOTA表现，包括将GLUE基准推高至80.4% (绝对改进7.6%)，MultiNLI准确度达到86.7% (绝对改进5.6%)，成为NLP发展史上的里程碑式的模型成就。\n",
    "[3]BERT是一种基于多层Transformer编码器的预训练语言模型，通过结合Tokenization、多种Embeddings和特定任务的输出层，能够捕捉文本的双向上下文信息，并在各种自然语言处理任务中表现出色1。 BERT的基本架构包括一个屏蔽语言模型和两个全连接层2。 在预训练阶段，BERT使用大规模的无标签文本数据，通过预测被屏蔽的单词或句子来学习语言表示2。 在应用阶段，BERT可以作为特征提取器，将输入文本转化为固定维度的向量表示，供下游任务使用Ro BERT a（Robustly Optimized BERT Pretraining Approach）是Facebook AI在BERT基础上提出的一种预训练语言模型。 与BERT相比，RoBERTa在模型结构、预训练策略、训练数据和训练方法等方面进行了深入的改进和优化，旨在提高模型的性能和泛化能力。 本文将详细介绍RoBERTa的原理和优势，并探讨其与BERT的不同之处。 从模型结构上讲，RoBERTa与BERT基本相同，都采用了Transformer架构。 然而，RoBERTa在模型规模上进行了扩展，使用了更多的参数和更大的模型容量。 这种扩展有助于提高模型的表示能力和泛化能力。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aceed343-ca6d-4c64-b6de-67b5ad7a3fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(chat(query_split_prompt, debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161224f4-9eab-4f8c-a218-2e730f8743a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"status_code\": 200, \"request_id\": \"0c426ba4-017d-9807-b343-2cb421e7c2f1\", \"code\": \"\", \"message\": \"\", \"output\": {\"text\": null, \"finish_reason\": null, \"choices\": [{\"finish_reason\": \"stop\", \"message\": {\"role\": \"assistant\", \"content\": \"\\n {\\n“question\\\": “苹果和华为谁更好？”,\\n“question_type”: “complex”,\\n“sub_quesiton”: [\\n{\\n“sub_question\\\": “苹果的产品线丰富吗？”,\\n“question_type”: “simple”\\n},\\n{\\n“sub_question\\\": “华为的产品线丰富吗？”,\\n“question_type”: “simple”\\n},\\n{\\n“sub_question\\\": “苹果的产品质量和性能如何？”,\\n“question_type”: “simple”\\n},\\n{\\n“sub_question\\\": “华为的产品质量和性能如何？”,\\n“question_type”: “simple”\\n},\\n{\\n“sub_question\\\": “苹果和华为在哪些方面有差异？”,\\n“question_type”: “complex”\\n}\\n]\\n}\"}}]}, \"usage\": {\"input_tokens\": 336, \"output_tokens\": 178, \"total_tokens\": 514}}\n"
     ]
    }
   ],
   "source": [
    "from http import HTTPStatus\n",
    "from dashscope import Generation\n",
    "\n",
    "import dashscope\n",
    "dashscope.api_key=\"skl-22ya1f77c712d426bb56c227d5ed44299\"\n",
    "\n",
    "def call_with_messages(prompt):\n",
    "    messages = [\n",
    "        {'role': 'system', 'content':'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': prompt}]\n",
    "    gen = Generation()\n",
    "    response = gen.call(\n",
    "        'chatglm3-6b',\n",
    "        messages=messages,\n",
    "        result_format='message',  # set the result is message format.\n",
    "    )\n",
    "    print(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    call_with_messages(query_split_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baadae20-9a5a-430e-8291-19030cc8f510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: export: `dashscope.api_key=sk-22a1f77c712d426bb56c227d5ed44299': not a valid identifier\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69774d82-0295-4362-879b-9c433dd75f2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
